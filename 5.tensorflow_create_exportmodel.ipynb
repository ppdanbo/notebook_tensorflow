{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "static-analysis",
   "metadata": {},
   "source": [
    "Introduction\n",
    "You can serve your TensorFlow models using a system called TensorFlow Serving. Serving a model means that clients can access models to make predictions through an API. TensorFlow Serving supports serving TensorFlow models out-of-the-box and is designed for use in production environments.\n",
    "\n",
    "To serve a model, you can use the tensorflow_model_server binary that is included in the Amazon Deep Learning AMI. You need to serialize your model for TensorFlow Serving to be able to serve it. TensorFlow includes the SavedModelBuilder module to simplify the process. You need to tell the builder the signature of the prediction model. The signature tells the builder the type and shape of the inputs and outputs of the model. The model is serialized and saved to disk using Google's protocol buffer serialization format and produces files with a .pb file extension. TensorFlow Serving supports versioning allowing you to easily serve multiple versions of a model.\n",
    "\n",
    "The graph you have worked with until now needs to be modified to support serving. The graph had been using constant inputs and was focused on training. When you serve a model you need to use placeholders for inputs so that a client can provide whatever input values they need to make predictions for. The code in this Lab Step indicates tensors used for training by appending train_ to their name. The new graph has separate paths for training and making predictions.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Export single neuron neural network model for TensorFlow Serving'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personalized-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\dbwab\\.conda\\envs\\ztdl\\lib\\site-packages\\ipykernel_launcher.py:\n",
      "  --export_dir: Export model directory\n",
      "    (default: 'C:\\\\Users\\\\dbwab\\\\awslab\\\\tensorflows\\\\models')\n",
      "  --model_version: Model version number\n",
      "    (default: '1')\n",
      "    (an integer)\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_integer('model_version', 1, 'Model version number')\n",
    "tf.app.flags.DEFINE_string('export_dir', 'C:\\\\Users\\\\dbwab\\\\awslab\\\\tensorflows\\\\models', 'Export model directory')\n",
    "#tf.app.flags.DEFINE_string('export_dir', '/tmp/nn', 'Export model directory')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recorded-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up sample points perturbed away from the ideal linear relationship\n",
    "# y = 0.5*x + 2.5\n",
    "num_examples = 60\n",
    "points = np.array([np.linspace(-1, 5, num_examples),\n",
    "                   np.linspace(2, 5, num_examples)])\n",
    "points += np.random.randn(2, num_examples)\n",
    "train_x, train_y = points\n",
    "# Include a 1 to use as the bias input for neurons\n",
    "train_x_with_bias = np.array([(1., d) for d in train_x]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expected-testimony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to C:\\Users\\dbwab\\awslab\\tensorflows\\models\\v1\n",
      "WARNING:tensorflow:From <ipython-input-6-58e30fe1973a>:47: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From <ipython-input-6-58e30fe1973a>:67: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: C:\\Users\\dbwab\\awslab\\tensorflows\\models\\v1\\saved_model.pb\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "training_steps = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Set up all the tensors, variables, and operations.\n",
    "  input = tf.constant(train_x_with_bias)\n",
    "  target = tf.constant(np.transpose([train_y]).astype(np.float32))\n",
    "  # Set up placeholder for making model predictions (separate from training)\n",
    "  x = tf.placeholder('float', shape=[None, 1])\n",
    "  # Initialize weights with small random values\n",
    "  weights = tf.Variable(tf.random_normal([2, 1], 0, 0.1))\n",
    "\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  # Calculate the current prediction error\n",
    "  train_y_predicted = tf.matmul(input, weights)\n",
    "  train_y_error = tf.subtract(train_y_predicted, target)\n",
    "\n",
    "  # Define prediction operation\n",
    "  y = tf.matmul(x, weights[1:]) + weights[0]\n",
    "\n",
    "  # Compute the L2 loss function of the error\n",
    "  loss = tf.nn.l2_loss(train_y_error)\n",
    "\n",
    "  # Train the network using an optimizer that minimizes the loss function\n",
    "  update_weights = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(loss)\n",
    "\n",
    "  for _ in range(training_steps):\n",
    "    # Repeatedly run the operations, updating the TensorFlow variable.\n",
    "    update_weights.run()\n",
    "\n",
    "  ## Export the Model\n",
    "\n",
    "  # Create a SavedModelBuilder\n",
    "  #export_path_base = FLAGS.export_dir\n",
    "  #export_path = os.path.join(export_path_base, str(FLAGS.model_version))\n",
    "  \n",
    "  export_path_base = \"C:\\\\Users\\\\dbwab\\\\awslab\\\\tensorflows\\\\models\" \n",
    "    \n",
    "  export_path = os.path.join(export_path_base, \"v1\")\n",
    "  print('Exporting trained model to', export_path)\n",
    "  builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "  \n",
    "  # Build signature inputs and outputs\n",
    "  tensor_info_input = tf.saved_model.utils.build_tensor_info(x)\n",
    "  tensor_info_output = tf.saved_model.utils.build_tensor_info(y)\n",
    "\n",
    "  # Create the prediction signature\n",
    "  prediction_signature = (\n",
    "    tf.saved_model.signature_def_utils.build_signature_def(\n",
    "      inputs={'input': tensor_info_input},\n",
    "      outputs={'output': tensor_info_output},\n",
    "      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "  # Provide legacy initialization op for compatibility with older version of tf\n",
    "  legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "\n",
    "  # Build the model\n",
    "  builder.add_meta_graph_and_variables(\n",
    "    sess, [tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "      'prediction':\n",
    "      prediction_signature,\n",
    "    },\n",
    "  legacy_init_op=legacy_init_op)\n",
    "\n",
    "  # Save the model\n",
    "  builder.save()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-emergency",
   "metadata": {},
   "source": [
    "The code for building and saving the model is at the bottom of the code under the ## Export the Model comment. You can also review the changes to the graph and notice the tf.placeholder x that is used as an input for making predictions. The placeholder is used by the prediction operation y = tf.matmul(x, weights[1:]) + weights[0]. Also notice the training operations are separated out by seeing how the train_ variables are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-selling",
   "metadata": {},
   "source": [
    "**CMD to serve the module:**\n",
    "    tensorflow_model_server --port=9000 --model_name=nn --model_base_path=/tmp/nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-faith",
   "metadata": {},
   "source": [
    "Summary\n",
    "In this Lab Step, you saw how to modify the neural network's graph to make separate operation paths for training and predicting. You also saw how to use the SavedModelBuilder module to save a serialized model to disk. Lastly, you used the tensorflow_model_server to serve the model making it accessible to clients to make predictions with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
