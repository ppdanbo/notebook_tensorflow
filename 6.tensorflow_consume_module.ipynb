{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cathedral-trick",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "TensorFlow Serving models are consumed by clients. Client code needs to make a connection to the server and communicate using protocol buffers. Google's gRPC remote procedure call library provides the ability to connect and make calls to the server. After establishing a connection, you make a call by preparing the input for model prediction and serializing it in the request. The serialized prediction output is returned by the server.\n",
    "\n",
    "In this Lab Step, you will use Python client code to request predictions from the model being served by TensorFlow Serving in the previous Lab Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empty-sydney",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_serving'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1311b9c44e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_serving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredict_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_serving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprediction_service_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_serving'"
     ]
    }
   ],
   "source": [
    "from grpc.beta import implementations\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "\n",
    "hostport = 'localhost:9000'\n",
    "\n",
    "\n",
    "def do_prediction(hostport):\n",
    "\n",
    "  # Create connection\n",
    "  host, port = hostport.split(':')\n",
    "  channel = implementations.insecure_channel(host, int(port))\n",
    "  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "  # Initialize a request\n",
    "  request = predict_pb2.PredictRequest()\n",
    "  request.model_spec.name = 'nn'\n",
    "  request.model_spec.signature_name = 'prediction'\n",
    "\n",
    "  # Use evenly-spaced points for test data\n",
    "  tests = temp_data = np.array([range(-1, 6, 1)]).transpose().astype(\n",
    "    np.float32)\n",
    "\n",
    "  # Set the tests as the input for prediction\n",
    "  request.inputs['input'].CopyFrom(\n",
    "    tf.contrib.util.make_tensor_proto(tests, shape=tests.shape))\n",
    "\n",
    "  # Get prediction from server\n",
    "  result = stub.Predict(request, 5.0) # 5 second timeout\n",
    "\n",
    "  # Compare to noise-free actual values\n",
    "  actual = np.sum(0.5 * temp_data + 2.5, 1)\n",
    "\n",
    "  return result, actual\n",
    "\n",
    "\n",
    "prediction, actual = do_prediction(hostport)\n",
    "print('Prediction is: ', prediction)\n",
    "print('Noise-free value is: ', actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-donor",
   "metadata": {},
   "source": [
    "The float_vals in the prediction output protocol buffer are the predicted values for each of the inputs you requested. Your values will differ from the image above, but you should see that they are close to the noise-free values.\n",
    "\n",
    " \n",
    "\n",
    "**Summary\n",
    "In this Lab Step, you created a TensorFlow Serving client that makes predictions using the model served in the previous Lab Step.\n",
    "\n",
    "You have now seen a comprehensive view of working with TensorFlow using the Amazon Deep Learning AMI. You developed a model in TensorFlow, analyzed the learning process in TensorBoard, served the trained model with TensorFlow Serving, and consumed the model using Python client code. You would follow the same steps for adding your own model into production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
